{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea60cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:06:38.817353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-14 15:06:38.984393: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-14 15:06:38.991362: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-14 15:06:38.991381: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-04-14 15:06:39.907332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-14 15:06:39.907462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-14 15:06:39.907470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:06:46,530 - py.warnings - WARNING - /usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usual imports\n",
    "import secml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "# SecML\n",
    "from secml.ml.features.normalization import CNormalizerMinMax\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.array import CArray\n",
    "from secml.ml.classifiers import CClassifierPyTorch\n",
    "\n",
    "# RobustBench\n",
    "import robustbench\n",
    "from robustbench.utils import load_model\n",
    "from secml.utils import fm\n",
    "from secml import settings\n",
    "\n",
    "# Albi utils\n",
    "from utils_attacks import *\n",
    "from utils_CP import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d873816",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1980a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.data.loader.c_dataloader_cifar import CDataLoaderCIFAR10\n",
    "lr,_ = CDataLoaderCIFAR10().load()\n",
    "\n",
    "n_tr = 1000  # Number of training set samples\n",
    "n_val = 50  # Number of validation set samples\n",
    "n_ts = 5000 # Number of test set samples\n",
    "n_cl = 4500 # Number of calibration set samples\n",
    "\n",
    "n = n_tr + n_val + n_cl + n_ts\n",
    "\n",
    "# Shuffle before splitting\n",
    "random_state = 999\n",
    "rng = np.random.default_rng(seed=random_state)\n",
    "shuffled_indices = rng.permutation(lr.X.shape[0]).tolist()\n",
    "lr = lr[shuffled_indices, :]\n",
    "\n",
    "# Split the dataset\n",
    "tr = lr[:n_tr, :]\n",
    "vl = lr[n_tr:n_tr + n_val, :]\n",
    "cl = lr[n_tr + n_val:n_tr + n_val + n_cl, :]\n",
    "ts = lr[n_tr + n_val + n_cl:n, :]\n",
    "\n",
    "# Normalize the features in `[0, 1]`\n",
    "tr.X /= 255\n",
    "vl.X /= 255\n",
    "ts.X /= 255\n",
    "cl.X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a402b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [0,1,2,3,4,5,6,7,8,9]\n",
    "dataset_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91b36c",
   "metadata": {},
   "source": [
    "### Load a Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a0511",
   "metadata": {},
   "source": [
    "#### Wang2023Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6768479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fm.join(settings.SECML_MODELS_DIR, 'robustbench')\n",
    "model_wang = load_model(model_name='Wang2023Better_WRN-70-16', norm='L2', model_dir=output_dir)\n",
    "clf_wang = CClassifierPyTorch(model_wang, input_shape=(3,32,32), pretrained=True, softmax_outputs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b341fe",
   "metadata": {},
   "source": [
    "#### Standard: WRS 28-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d4e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fm.join(settings.SECML_MODELS_DIR, 'robustbench')\n",
    "model_std = load_model(model_name='Standard', norm='Linf', model_dir=output_dir)\n",
    "clf_std = CClassifierPyTorch(model_std, input_shape=(3,32,32), pretrained=True, softmax_outputs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c80365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def evaluate_attacks(model_name, norm_name, attack_configs, cl, ts, clf, alpha=0.1, base_output_dir=\"./Results\"):\n",
    "    \"\"\"\n",
    "    Evaluate different adversarial attacks on a given model and dataset.\n",
    "    \"\"\"\n",
    "    n_cl = cl.X.shape[0]\n",
    "    \n",
    "    # Prepare output directory\n",
    "    save_path = os.path.join(base_output_dir, model_name, norm_name)\n",
    "    if os.path.exists(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    csv_file = os.path.join(save_path, \"results_all.csv\")\n",
    "    \n",
    "    # Run attacks\n",
    "    cl_att_dict = attack_dataset(cl, clf, attack_configs, desc=\"Running attacks\", n_jobs=1)\n",
    "    ts_att_dict = attack_dataset(ts, clf, attack_configs, desc=\"Running attacks\", n_jobs=1)\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    \n",
    "    if isinstance(cl_att_dict, dict):\n",
    "        attack_types = cl_att_dict.keys()\n",
    "    else:\n",
    "        attack_types = [get_single_attack_key(attack_configs[0])]\n",
    "        cl_att_dict = {attack_types[0]: cl_att_dict}\n",
    "        ts_att_dict = {attack_types[0]: ts_att_dict}\n",
    "    \n",
    "    for attack_type in attack_types:\n",
    "        cl_att = cl_att_dict[attack_type]\n",
    "        ts_att = ts_att_dict[attack_type]\n",
    "        \n",
    "        cl_att_scores = compute_score(cl, cl_att, clf)\n",
    "        cl_scores = compute_score(cl, cl, clf)\n",
    "        \n",
    "        # Compute quantiles\n",
    "        q_level = np.ceil((n_cl + 1) * (1 - alpha)) / n_cl\n",
    "        qhat = np.quantile(cl_scores, q_level, method='higher')\n",
    "        qhat_A = np.quantile(cl_att_scores, q_level, method='higher')\n",
    "        \n",
    "        # Compute conformal sets\n",
    "        att_conformal_sets,_ = compute_CP(ts_att, qhat_A, clf)\n",
    "        cs_conformal_sets,_ = compute_CP(ts_att, qhat, clf)\n",
    "        \n",
    "        # Compute coverage and variance\n",
    "        att_coverage = compute_covergae(ts, att_conformal_sets)\n",
    "        att_coverage_var = compute_covergae_std(ts, att_conformal_sets)\n",
    "        cs_coverage = compute_covergae(ts, cs_conformal_sets)\n",
    "        cs_coverage_var = compute_covergae_std(ts, cs_conformal_sets)\n",
    "        \n",
    "        # Compute mean and variance of set sizes\n",
    "        att_size_mean = mean_conformal_sets(att_conformal_sets)\n",
    "        att_size_var = std_conformal_sets(att_conformal_sets) / 10\n",
    "        cs_size_mean = mean_conformal_sets(cs_conformal_sets)\n",
    "        cs_size_var = std_conformal_sets(cs_conformal_sets) / 10\n",
    "        \n",
    "        results.append({\n",
    "            \"attack_type\": \"Vanilla\",\n",
    "            \"coverage\": f\"{cs_coverage:.4f} ± {cs_coverage_var:.4f}\",\n",
    "            \"size\": f\"{cs_size_mean:.4f} ± {cs_size_var:.4f}\"\n",
    "        })\n",
    "        \n",
    "        results.append({\n",
    "            \"attack_type\": attack_type,\n",
    "            \"coverage\": f\"{att_coverage:.4f} ± {att_coverage_var:.4f}\",\n",
    "            \"size\": f\"{att_size_mean:.4f} ± {att_size_var:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"attack_type\", \"coverage\", \"size\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"Results saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfc200",
   "metadata": {},
   "source": [
    "## Run them all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf5b70",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running attacks:   0%|          | 0/500 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:11:05,252 - py.warnings - WARNING - /home/acarlevaro/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running attacks: 100%|██████████| 500/500 [3:02:45<00:00, 21.93s/sample]  \n",
      "Running attacks:  21%|██        | 42/200 [16:35<34:37, 13.15s/sample]  "
     ]
    }
   ],
   "source": [
    "model_name = \"Standard\"\n",
    "\n",
    "eps_inf = 8/255\n",
    "\n",
    "attack_configs_linf = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": eps_inf, \"step_size\": eps_inf/10, \"steps\": 10, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": eps_inf, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "\n",
    "]\n",
    "\n",
    "evaluate_attacks(model_name, \"Linf\", attack_configs_linf, cl, ts, clf_std, alpha=0.1, base_output_dir=\"./Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b29dc5",
   "metadata": {},
   "source": [
    "#### Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a35e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Wang\"\n",
    "\n",
    "attack_configs = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": 0.5, \"step_size\": 0.5/10, \"steps\": 10, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": 0.5, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "    {\"attack_type\": \"CW\"}\n",
    "    #{\"attack_type\": \"DDN\", \"epsilon\": 0.5, \"init_epsilon\":0.01, \"gamma\":0.01, \"steps\":50, \"lb\":0.0, \"ub\":1.0},\n",
    "    #{\"attack_type\": \"EAD\", \"epsilon\": 0.5, \"binary_search_steps\":15, \"initial_stepsize\":0.01, \"confidence\":0.0, \"initial_const\":0.01, \"regularization\":0.1, \"steps\":10, \"lb\":0.0, \"ub\":1.0 }\n",
    "]\n",
    "\n",
    "\n",
    "evaluate_attacks(model_name, \"L2\", attack_configs, cl, ts, clf_wang, alpha=0.1, base_output_dir=\"./Results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
