{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 22:16:20.266355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-16 22:16:20.444292: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 22:16:20.449861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-16 22:16:20.449877: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-04-16 22:16:21.290187: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-16 22:16:21.290286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-04-16 22:16:21.290293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Usual imports\n",
    "import secml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torchvision import datasets, transforms\n",
    "import shutil\n",
    "import json\n",
    "from PIL import Image\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "# SecML\n",
    "from secml.ml.features.normalization import CNormalizerMinMax\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.array import CArray\n",
    "from secml.data import CDataset\n",
    "from secml.ml.classifiers import CClassifierPyTorch\n",
    "\n",
    "# RobustBench\n",
    "import robustbench\n",
    "from robustbench.utils import load_model\n",
    "from secml.utils import fm\n",
    "from secml import settings\n",
    "\n",
    "# Albi utils\n",
    "from utils_attacks import *\n",
    "from utils_CP import *\n",
    "from utils_train_tinyimagenet import *\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdf871",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4c456",
   "metadata": {},
   "source": [
    "#### Procedure to download, split and preprocess ImageNet (done once, then pickle upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38ba724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_learning shape: (10000, 3, 224, 224), y_learning shape: (10000,)\n",
      "CDataset: CDataset{'X': CArray(10000, 150528)(dense: [[0.992157 0.992157 0.992157 ... 0.992157 0.992157 0.996078] [0.745098 0.737255 0.733333 ... 0.607843 0.607843 0.603922] [0.337255 0.32549  0.294118 ... 0.066667 0.047059 0.027451] ... [0.270588 0.301961 0.337255 ... 0.286275 0.286275 0.298039] [0.254902 0.254902 0.235294 ... 0.14902  0.145098 0.141176] [0.509804 0.513726 0.517647 ... 0.121569 0.152941 0.047059]]), 'Y': CArray(10000,)(dense: [885 520 195 ... 241 481 850]), 'header': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the root directory containing both the validation images and devkit\n",
    "#dataset_path = \"/home/acarlevaro/Sources/albi/Adversarial_CP_V3/InyImageNet\"\n",
    "\n",
    "# Define the transforms (resize, tensor conversion, and normalization)\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize((224, 224)),  # Resize to 224x224 (ImageNet standard)\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "#])\n",
    "\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize(256),\n",
    "#    transforms.CenterCrop(224),\n",
    "#    transforms.ToTensor(),\n",
    "#])\n",
    "\n",
    "# Load ImageNet Validation Dataset\n",
    "#val_dataset = datasets.ImageNet(\n",
    "#    root=dataset_path,\n",
    "#    split='val',  # You can also use 'train' for the training dataset\n",
    "#    transform=transform\n",
    "#)\n",
    "\n",
    "# Example of iterating over the dataset\n",
    "#for img, label in val_dataset[:50,:]:\n",
    "#    print(img.size(), label)\n",
    "    \n",
    "dataset_path = \"/home/acarlevaro/Sources/albi/Adversarial_CP_V3/InyImageNet/ILSVRC2012_Albi\"\n",
    "\n",
    "# Define the transforms (resize, tensor conversion, and normalization)\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.Resize((224, 224)),  # Resize to 224x224 (ImageNet standard)\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "#])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Select a random subset of 10,000 samples\n",
    "num_samples = 10000\n",
    "subset_indices = random.sample(range(len(full_dataset)), num_samples)\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "learning_dataset = subset_dataset\n",
    "\n",
    "#X_train = np.array([train_dataset[i][0].numpy() for i in range(len(train_dataset))])\n",
    "#y_train = np.array([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "\n",
    "X_learning = np.array([learning_dataset[i][0].numpy() for i in range(len(learning_dataset))])\n",
    "y_learning = np.array([learning_dataset[i][1] for i in range(len(learning_dataset))])\n",
    "\n",
    "#print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")  \n",
    "print(f\"X_learning shape: {X_learning.shape}, y_learning shape: {y_learning.shape}\")  \n",
    "\n",
    "# Convert NumPy arrays to CArray\n",
    "X_learning_carray = CArray(X_learning.reshape(X_learning.shape[0], -1))  # Flatten images\n",
    "y_learning_carray = CArray(y_learning)\n",
    "\n",
    "# Create the CDataset\n",
    "lr = CDataset(X_learning_carray, y_learning_carray)\n",
    "\n",
    "# Print dataset summary\n",
    "print(\"CDataset:\", lr)\n",
    "\n",
    "# Shuffle before splitting\n",
    "random_state = 999\n",
    "rng = np.random.default_rng(seed=random_state)\n",
    "shuffled_indices = rng.permutation(lr.X.shape[0]).tolist()\n",
    "lr = lr[shuffled_indices, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bc6a0",
   "metadata": {},
   "source": [
    "#### Pickle file saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb97ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD_ILSVRC2012_Albi.pkl saved to pickle file.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the CDataset object\n",
    "with open('CD_ILSVRC2012_Albi.pkl', \"wb\") as f:\n",
    "    pickle.dump(lr, f)\n",
    "print(\"CD_ILSVRC2012_Albi.pkl saved to pickle file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98388ace",
   "metadata": {},
   "source": [
    "## Load dataset (pickle, ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7978e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDataset loaded successfully: CDataset{'X': CArray(10000, 150528)(dense: [[0.956863 0.952941 0.956863 ... 0.078431 0.105882 0.145098] [0.137255 0.152941 0.117647 ... 0.929412 0.823529 0.498039] [0.333333 0.305882 0.337255 ... 0.105882 0.082353 0.12549 ] ... [0.811765 0.811765 0.815686 ... 0.752941 0.756863 0.721569] [0.560784 0.552941 0.560784 ... 0.27451  0.282353 0.27451 ] [0.494118 0.505882 0.501961 ... 0.835294 0.819608 0.662745]]), 'Y': CArray(10000,)(dense: [208 102 898 ... 591 467 941]), 'header': None}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the CDataset object\n",
    "with open('CD_ILSVRC2012_Albi.pkl', \"rb\") as f:\n",
    "    lr = pickle.load(f)\n",
    "\n",
    "# Print dataset summary to verify\n",
    "print(\"CDataset loaded successfully:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8115d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split sizes\n",
    "n_tr = 5000  # Training set\n",
    "n_val = 50   # Validation set\n",
    "n_ts = 1000  # Test set\n",
    "n_cl = 2500  # Calibration set\n",
    "n = n_tr + n_val + n_cl + n_ts\n",
    "\n",
    "# Split dataset\n",
    "tr = lr[:n_tr, :]\n",
    "vl = lr[n_tr:n_tr + n_val, :]\n",
    "cl = lr[n_tr + n_val:n_tr + n_val + n_cl, :]\n",
    "ts = lr[n_tr + n_val + n_cl:n, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a71f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150528"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224*224*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a652d",
   "metadata": {},
   "source": [
    "## Load a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0009e1",
   "metadata": {},
   "source": [
    "### Salman, ResNet-18, NeurIPS2020, RA: 25.32%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8612e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fm.join(settings.SECML_MODELS_DIR, 'robustbench')\n",
    "model_Salman = load_model(model_name='Salman2020Do_R18', dataset='imagenet', threat_model='Linf', model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4217a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 50.00%\n"
     ]
    }
   ],
   "source": [
    "from secml.ml.features import CNormalizerMeanStd\n",
    "\n",
    "# imagenet normalization\n",
    "#normalizer = CNormalizerMeanStd(mean=(0.485, 0.456, 0.406),\n",
    "#                                std=(0.229, 0.224, 0.225))\n",
    "\n",
    "clf_Salman = CClassifierPyTorch(model = model_Salman,\n",
    "                         input_shape=(3,224,224),\n",
    "                         pretrained=True,\n",
    "                         #preprocess=normalizer,\n",
    "                         softmax_outputs = True)\n",
    "\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# Compute predictions on a test set\n",
    "y_pred = clf_Salman.predict(ts.X)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b772f",
   "metadata": {},
   "source": [
    "### Liu, Swin-L, arXivFeb2023, RA: 56.56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731f9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fm.join(settings.SECML_MODELS_DIR, 'robustbench')\n",
    "model_Liu2023 = load_model(model_name='Liu2023Comprehensive_ConvNeXt-B', dataset='imagenet', threat_model='Linf', model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87783602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 74.60%\n"
     ]
    }
   ],
   "source": [
    "from secml.ml.features import CNormalizerMeanStd\n",
    "\n",
    "# imagenet normalization\n",
    "#normalizer = CNormalizerMeanStd(mean=(0.485, 0.456, 0.406),\n",
    "#                                std=(0.229, 0.224, 0.225))\n",
    "\n",
    "clf_Liu = CClassifierPyTorch(model = model_Liu2023,\n",
    "                         input_shape=(3,224,224),\n",
    "                         pretrained=True,\n",
    "                         #preprocess=normalizer,\n",
    "                         softmax_outputs = True)\n",
    "\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# Compute predictions on a test set\n",
    "y_pred = clf_Liu.predict(ts.X)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbb8fe",
   "metadata": {},
   "source": [
    "### TIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ea0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = fm.join(settings.SECML_MODELS_DIR, 'robustbench')\n",
    "model_Tian = load_model(model_name='Tian2022Deeper_DeiT-B', dataset='imagenet', threat_model='corruptions', model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6f68afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 74.60%\n"
     ]
    }
   ],
   "source": [
    "from secml.ml.features import CNormalizerMeanStd\n",
    "\n",
    "# imagenet normalization\n",
    "#normalizer = CNormalizerMeanStd(mean=(0.485, 0.456, 0.406),\n",
    "#                                std=(0.229, 0.224, 0.225))\n",
    "\n",
    "clf_Tian = CClassifierPyTorch(model = model_Tian,\n",
    "                         input_shape=(3,224,224),\n",
    "                         pretrained=True,\n",
    "                         #preprocess=normalizer,\n",
    "                         softmax_outputs = True)\n",
    "\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# Compute predictions on a test set\n",
    "y_pred = clf_Liu.predict(ts.X)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec71b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def evaluate_attacks(model_name, norm_name, attack_configs, cl, ts, clf, alpha=0.1, base_output_dir=\"./Results\"):\n",
    "    \"\"\"\n",
    "    Evaluate different adversarial attacks on a given model and dataset.\n",
    "    \"\"\"\n",
    "    n_cl = cl.X.shape[0]\n",
    "    \n",
    "    # Prepare output directory\n",
    "    save_path = os.path.join(base_output_dir, model_name, norm_name)\n",
    "    if os.path.exists(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    csv_file = os.path.join(save_path, \"results_all.csv\")\n",
    "    \n",
    "    # Run attacks\n",
    "    cl_att_dict = attack_dataset(cl, clf, attack_configs, desc=\"Running attacks\", n_jobs=1)\n",
    "    ts_att_dict = attack_dataset(ts, clf, attack_configs, desc=\"Running attacks\", n_jobs=1)\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    \n",
    "    if isinstance(cl_att_dict, dict):\n",
    "        attack_types = cl_att_dict.keys()\n",
    "    else:\n",
    "        attack_types = [get_single_attack_key(attack_configs[0])]\n",
    "        cl_att_dict = {attack_types[0]: cl_att_dict}\n",
    "        ts_att_dict = {attack_types[0]: ts_att_dict}\n",
    "    \n",
    "    for attack_type in attack_types:\n",
    "        cl_att = cl_att_dict[attack_type]\n",
    "        ts_att = ts_att_dict[attack_type]\n",
    "        \n",
    "        cl_att_scores = compute_score(cl, cl_att, clf)\n",
    "        cl_scores = compute_score(cl, cl, clf)\n",
    "        \n",
    "        # Compute quantiles\n",
    "        q_level = np.ceil((n_cl + 1) * (1 - alpha)) / n_cl\n",
    "        qhat = np.quantile(cl_scores, q_level, method='higher')\n",
    "        qhat_A = np.quantile(cl_att_scores, q_level, method='higher')\n",
    "        \n",
    "        # Compute conformal sets\n",
    "        att_conformal_sets,_ = compute_CP(ts_att, qhat_A, clf)\n",
    "        cs_conformal_sets,_ = compute_CP(ts_att, qhat, clf)\n",
    "        \n",
    "        # Compute coverage and variance\n",
    "        att_coverage = compute_covergae(ts, att_conformal_sets)\n",
    "        att_coverage_var = compute_covergae_std(ts, att_conformal_sets)\n",
    "        cs_coverage = compute_covergae(ts, cs_conformal_sets)\n",
    "        cs_coverage_var = compute_covergae_std(ts, cs_conformal_sets)\n",
    "        \n",
    "        # Compute mean and variance of set sizes\n",
    "        att_size_mean = mean_conformal_sets(att_conformal_sets)\n",
    "        att_size_var = std_conformal_sets(att_conformal_sets) / 10\n",
    "        cs_size_mean = mean_conformal_sets(cs_conformal_sets)\n",
    "        cs_size_var = std_conformal_sets(cs_conformal_sets) / 10\n",
    "        \n",
    "        results.append({\n",
    "            \"attack_type\": \"Vanilla\",\n",
    "            \"coverage\": f\"{cs_coverage:.4f} ± {cs_coverage_var:.4f}\",\n",
    "            \"size\": f\"{cs_size_mean:.4f} ± {cs_size_var:.4f}\"\n",
    "        })\n",
    "        \n",
    "        results.append({\n",
    "            \"attack_type\": attack_type,\n",
    "            \"coverage\": f\"{att_coverage:.4f} ± {att_coverage_var:.4f}\",\n",
    "            \"size\": f\"{att_size_mean:.4f} ± {att_size_var:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"attack_type\", \"coverage\", \"size\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"Results saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ba0ea",
   "metadata": {},
   "source": [
    "### Liu2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea67277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running attacks:   0%|          | 0/500 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 22:19:43,171 - py.warnings - WARNING - /home/acarlevaro/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Liu\"\n",
    "\n",
    "#attack_configs = [ \n",
    "#    {\"attack_type\": \"PGD\", \"epsilon\": 0.5, \"step_size\": 0.5/10, \"steps\": 10, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "#    {\"attack_type\": \"FGM\", \"epsilon\": 0.5, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "#    {\"attack_type\": \"DeepFool\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "#    {\"attack_type\": \"BasicIterative\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "#    {\"attack_type\": \"CW\"}\n",
    "#    #{\"attack_type\": \"DDN\", \"epsilon\": 0.5, \"init_epsilon\":0.01, \"gamma\":0.01, \"steps\":50, \"lb\":0.0, \"ub\":1.0},\n",
    "#    #{\"attack_type\": \"EAD\", \"epsilon\": 0.5, \"binary_search_steps\":15, \"initial_stepsize\":0.01, \"confidence\":0.0, \"initial_const\":0.01, \"regularization\":0.1, \"steps\":10, \"lb\":0.0, \"ub\":1.0 }\n",
    "#]\n",
    "\n",
    "#evaluate_attacks(model_name, \"L2\", attack_configs, cl, ts, clf_Liu, alpha=0.1, base_output_dir=\"./Results\")\n",
    "\n",
    "eps_inf = 4/255\n",
    "\n",
    "attack_configs_linf = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": eps_inf, \"step_size\": eps_inf/10, \"steps\": 10, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": eps_inf, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "\n",
    "]\n",
    "\n",
    "evaluate_attacks(model_name, \"Linf\", attack_configs_linf, cl, ts, clf_Liu, alpha=0.1, base_output_dir=\"./Results\")\n",
    "\n",
    "#eps_1 = 0.5*np.sqrt(224)\n",
    "\n",
    "#attack_configs_l1 = [ \n",
    "#    {\"attack_type\": \"PGD\", \"epsilon\": eps_1, \"step_size\": eps_1/10, \"steps\": 10, \"distance\": \"l1\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "#    {\"attack_type\": \"FGM\", \"epsilon\": eps_1, \"distance\": \"l1\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "#    {\"attack_type\": \"BasicIterative\", \"epsilon\": eps_1, \"distance\": \"l1\"},\n",
    "\n",
    "#]\n",
    "\n",
    "#evaluate_attacks(model_name, \"L1\", attack_configs, cl, ts, clf_Liu, alpha=0.1, base_output_dir=\"./Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7e644",
   "metadata": {},
   "source": [
    "### Salman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salman\"\n",
    "\n",
    "attack_configs = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": 0.5, \"step_size\": 0.5/10, \"steps\": 10, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": 0.5, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": 0.5, \"distance\": \"l2\"},\n",
    "    {\"attack_type\": \"CW\"}\n",
    "    #{\"attack_type\": \"DDN\", \"epsilon\": 0.5, \"init_epsilon\":0.01, \"gamma\":0.01, \"steps\":50, \"lb\":0.0, \"ub\":1.0},\n",
    "    #{\"attack_type\": \"EAD\", \"epsilon\": 0.5, \"binary_search_steps\":15, \"initial_stepsize\":0.01, \"confidence\":0.0, \"initial_const\":0.01, \"regularization\":0.1, \"steps\":10, \"lb\":0.0, \"ub\":1.0 }\n",
    "]\n",
    "\n",
    "\n",
    "evaluate_attacks(model_name, \"L2\", attack_configs, cl, ts, clf_Salman, alpha=0.1, base_output_dir=\"./Results\")\n",
    "\n",
    "eps_inf = 0.5/np.sqrt(224)\n",
    "\n",
    "attack_configs_linf = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": eps_inf, \"step_size\": eps_inf/10, \"steps\": 10, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": eps_inf, \"distance\": \"linf\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": eps_inf, \"distance\": \"linf\"},\n",
    "\n",
    "]\n",
    "\n",
    "evaluate_attacks(model_name, \"Linf\", attack_configs_linf, cl, ts, clf_Salman, alpha=0.1, base_output_dir=\"./Results\")\n",
    "\n",
    "eps_1 = 0.5*np.sqrt(224)\n",
    "\n",
    "attack_configs_l1 = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": eps_1, \"step_size\": eps_1/10, \"steps\": 10, \"distance\": \"l1\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": eps_1, \"distance\": \"l1\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": eps_1, \"distance\": \"l1\"},\n",
    "\n",
    "]\n",
    "\n",
    "evaluate_attacks(model_name, \"L1\", attack_configs, cl, ts, clf_Salman, alpha=0.1, base_output_dir=\"./Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef2ec1",
   "metadata": {},
   "source": [
    "### Tian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aae26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running attacks:   0%|          | 11/2500 [10:40<48:07:59, 69.62s/sample]"
     ]
    }
   ],
   "source": [
    "model_name = \"Tian\"\n",
    "\n",
    "attack_configs = [ \n",
    "    {\"attack_type\": \"PGD\", \"epsilon\": 0.5, \"step_size\": 0.5/10, \"steps\": 10, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"FGM\", \"epsilon\": 0.5, \"distance\": \"l2\", \"lb\": 0.0, \"ub\": 1.0},\n",
    "    {\"attack_type\": \"DeepFool\", \"epsilon\": 0.5, \"distance\": \"l2\", \"steps\": 10},\n",
    "    {\"attack_type\": \"BasicIterative\", \"epsilon\": 0.5, \"distance\": \"l2\", \"steps\": 10},\n",
    "    {\"attack_type\": \"CW\", \"steps\": 10}\n",
    "    #{\"attack_type\": \"DDN\", \"epsilon\": 0.5, \"init_epsilon\":0.01, \"gamma\":0.01, \"steps\":50, \"lb\":0.0, \"ub\":1.0},\n",
    "    #{\"attack_type\": \"EAD\", \"epsilon\": 0.5, \"binary_search_steps\":15, \"initial_stepsize\":0.01, \"confidence\":0.0, \"initial_const\":0.01, \"regularization\":0.1, \"steps\":10, \"lb\":0.0, \"ub\":1.0 }\n",
    "]\n",
    "\n",
    "\n",
    "evaluate_attacks(model_name, \"L2\", attack_configs, cl, ts, clf_Tian, alpha=0.1, base_output_dir=\"./Results\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
